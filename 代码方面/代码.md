

## 1.求相似度算法

~~~c++
#include <bits/stdc++.h>

using namespace std;
map<char, int> valueKey;

void keyInit() {
  valueKey['A'] = 0;
  valueKey['C'] = 1;
  valueKey['G'] = 2;
  valueKey['T'] = 3;
  valueKey['-'] = 4;
}

int mapping[5][5] = {5,  -1, -2, -1, -3, -1, 5,  -3, -2, -4, -2, -3, 5,
                     -2, -2, -1, -2, -2, 5,  -1, -3, -4, -2, -1, 0};
int dp[101][101];
int dpcnt[101][101];

int max(int x, int y, int z) {
  int temp = x > y ? x : y;
  return temp > z ? temp : z;
}

int min(int x, int y) { return x < y ? x : y; }

class DNA {
 public:
  string name;
  string s;

  DNA(string name, string s) : name(name), s(s) {}
};

double func(DNA a, DNA b) {
  int l1 = a.s.size();
  int l2 = b.s.size();
  string s1 = a.s;
  string s2 = b.s;
  dp[0][0] = 0;
  dpcnt[0][0] = 0;
  for (int i = 1; i <= l1; i++) {
    dp[i][0] = dp[i - 1][0] + mapping[valueKey[s1[i]]][valueKey['-']];
    dpcnt[i][0] = dpcnt[i - 1][0] + 1;
  }
  for (int j = 1; j <= l2; j++) {
    dp[0][j] = dp[0][j - 1] + mapping[valueKey['-']][valueKey[s2[j]]];
    dpcnt[0][j] = dpcnt[0][j - 1] + 1;
  }
  for (int i = 1; i <= l1; i++) {
    for (int j = 1; j <= l2; j++) {
      dp[i][j] =
          max(dp[i][j - 1] + mapping[valueKey['-']][valueKey[s2[j]]],
              dp[i - 1][j] + mapping[valueKey[s1[i]]][valueKey['-']],
              dp[i - 1][j - 1] + mapping[valueKey[s1[i]]][valueKey[s2[j]]]);
      if (dp[i][j] ==
          dp[i - 1][j - 1] + mapping[valueKey[s1[i]]][valueKey[s2[j]]])
        dpcnt[i][j] = dpcnt[i - 1][j - 1] + 1;
      else if (dp[i - 1][j] + mapping[valueKey[s1[i]]][valueKey['-']] ==
               dp[i][j - 1] + mapping[valueKey['-']][valueKey[s2[j]]]) {
        dpcnt[i][j] = min(dpcnt[i - 1][j], dpcnt[i][j - 1]) + 1;
      } else if (dp[i][j] ==
                 dp[i - 1][j] + mapping[valueKey[s1[i]]][valueKey['-']])
        dpcnt[i][j] = dpcnt[i - 1][j] + 1;
      else
        dpcnt[i][j] = dpcnt[i][j - 1] + 1;
    }
  }
  return 20.0 * dp[l1][l2] / dpcnt[l1][l2];
}
int main() {
  keyInit();
  freopen("a.txt", "r", stdin);
  freopen("out.txt", "w", stdout);
  string name, s;
  vector<DNA> a;
  while (cin >> name >> s) {
    DNA t = DNA(name, s);
    a.push_back(t);
  }
  for (int i = 0; i < a.size() - 1; i++) {
    for (int j = i + 1; j < a.size(); j++) {
      double t = func(a[i], a[j]);
        if(t>30){
                  cout << a[i].name << "\t\t" << a[j].name << "\t\t" << fixed
           << setprecision(2) << t << "%" << endl;
        }

      //printf("%s\t\t%s\t\t%.2f%%\n", a[i].name.c_str(), a[j].name.c_str(), t);
    }
  }
  return 0;
}


~~~

## 2.求21种疾病相关度算法

~~~c++
#include<bits/stdc++.h>
#include<cstring>
#include<string>
#include<stdio.h>

using namespace std;
const int N = 2e5 + 10;
int temp;
//主串与字串比较
bool sub(string s, string t) {

    string::size_type idx;
    idx = s.find(t); //在a中查找b.
    if (idx == string::npos) //不存在。
        return false;
    else return true;
}

int main() {

    string start[21] = {"Alzheimer Disease", "Arthritis, Rheumatoid", "Breast Neoplasms",
                        "Carcinoma, Non-Small-Cell Lung", "Carcinoma, Renal Cell", "Cardiovascular Abnormalities",
                        "Colonic Neoplasms", "Endometrial Neoplasms", "Head and Neck Neoplasms", "Infertility, Male",
                        "Leukemia", "Lung Neoplasms", "Ovarian Neoplasms", "Pancreatic Neoplasms",
                        "Testicular Neoplasms", "Stomach Neoplasms", "Prostatic Neoplasms", "Stroke",
                        "Carcinoma, Hepatocellular", "Adenocarcinoma", "Carcinoma, Squamous Cell"};
    freopen("in1.txt", "r", stdin);
    freopen("out1.txt", "w", stdout);
    string a;
    string s[N], is[50];//is存几个相等的
    int i = 0, sum = 0, count = 0;
    bool iss[21];
    for (int k = 0; k < 21; k++) {
        iss[i] = false;
    }
    while (getline(cin, a)) {
        s[i] = a;
        i++;
    }
    sum = i;//sum是s[i]的长度，是主串
    //44是子串,可以求任意一个
    for (int i = 0; i < sum; i++) {
        count = 0;
        int sumry = 0;
        for (int j = 0; j < 21; j++)
            if (sub(s[i], start[j])) {
                is[count] = start[j];
                count++;
                iss[j] = true;
            }


//        if (count == 2) {
//
//            for (int j = 0; j < count; j++)
//                sumry += is[j].size();
//            if (sumry + 23 > s[i].size()) {
//                cout << s[i] << endl;
//            }
//        }
//
    }
    for (int j = 0; j < 21; j++) {
        if (iss[j]) {
            cout << start[j] << "---------" << j << endl;
        }
    }

    return 0;
}
~~~

## 3.判断AGCT

### 转换成24种数字序列

~~~c++
//16种情况
#include<bits/stdc++.h>

using namespace std;

class xulie {
public:
    int a, b, c, d;

    xulie(int a, int b, int c, int d) : a(a), b(b), c(c), d(d) {}
};

int main() {

    int n=4, i;
    int aa[24][4];
    int jj=0;
    int num[4]={1,2,3,4};                         //先定义一个n各元素的数组—按升序（降序）
    for (i = 0; i < n; i++)
        num[i] = i + 1;
    do {
        for (i = 0; i < n; i++)
        {
            aa[jj][i]=num[i];
            //输出第一个序列
            // cout<<num[i]<<" ";

        }
        //   cout<<endl;
        jj++;
    } while (next_permutation(num, num + n));//用函数求下（上）一个序列

    freopen("in.txt", "r", stdin);
    freopen("out.txt", "w", stdout);
    int a, b, c, d;
    int count = 0;
    vector<xulie> sss;
    while (cin >> a >> b >> c >> d) {
        xulie as = xulie(a, b, c, d);
        sss.push_back(as);
    }
    for(jj=0;jj<24;jj++){
        for (int i = 0; i < sss.size(); i++) {
            if (sss[i].a == 1)
                cout << aa[jj][0];
            else if (sss[i].b == 1)
                cout << aa[jj][1];
            else if (sss[i].c == 1)
                cout << aa[jj][2];
            else if (sss[i].d==1)
                cout << aa[jj][3];
        }
        cout<<endl;
    }
}
~~~

### 求得24种序列数组

~~~c++
#include<iostream>
using namespace std;
int main(){
    char s;

    freopen("in.txt", "r", stdin);
    freopen("out.txt", "w", stdout);
    int i=0,k;
    string ss[24];  //求的ss数组
    k=0;
    while(cin>>s){
        if(s=='1') ss[k]+='A';
        else if(s=='2') ss[k]+='G';
        else if(s=='3') ss[k]+='C';
        else ss[k]+='T';
        i++;
        if(i%31==0){ //实际长度
          k++;
        }
    }
    for(i=0;i<16;i++)
        cout<<ss[i]<<endl;
    return 0;
}

~~~

## 4.拓扑特征算法

### 已经有5个所求拓扑特征

~~~python
1.nx.degree(G)           # 计算节点的度。

#计算图的密度，其值为边数m除以图中可能边数（即n(n-1)/2）

2.nx.clustering(G)       # 网络节点的聚类系数。计算公式为：节点u的两个邻居节点间的边数除以((d(u)(d(u)-1)/2)。

3.nx.degree_centrality(G)    # 节点度中心系数。通过节点的度表示节点在图中的重要性，默认情况下会进行归一化，其值表达为节点度d(u)除以n-1（其中n-1就是归一化使用的常量）。这里由于可能存在循环，所以该值可能大于1.

4.nx.closeness_centrality(G)     # 节点距离中心系数。通过距离来表示节点在图中的重要性，一般是指节点到其他节点的平均路径的倒数，这里还乘以了n-1。该值越大表示节点到其他节点的距离越近，即中心性越高。

5.nx.betweenness_centrality(G)    # 节点介数中心系数。在无向图中，该值表示为节点作占最短路径的个数除以((n-1)(n-2)/2)；在有向图中，该值表达为节点作占最短路径个数除以((n-1)(n-2))。

#nx.transitivity(G)        # 图或网络的传递性。即图或网络中，认识同一个节点的两个节点也可能认识双方，计算公式为3*图中三角形的个数/三元组个数（该三元组个数是有公共顶点的边对数，这样就好数了）。

#nx.diameter(G)        # 网络直径’

#  一个值        nx.average_shortest_path_length(G)    # 网络最短路径

#degree = nx.degree_histogram(G)          #返回图中所有节点的度分布序列

~~~



## 5.加权随机森林代码

~~~python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import time
import numpy as np
from numpy import linalg as LA
from sklearn.model_selection import train_test_split
from sklearn.model_selection import ShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import confusion_matrix

from scipy import stats

import sys
from os import listdir
from os.path import isfile, join

#判断是否输入了4个参数（个人注释）

if(len(sys.argv) != 4):
    sys.exit("Usage: %s directory-for-input-files outdir randomSeed\n" %(sys.argv[0]))

#参数计算，定义calPRAUC函数，计算查准率，查全率，PR曲线（个人注释）   
    
def calPRAUC(ranks, nTPs, topN):
    cumPRAUC = 0
    posRecalls = list()
    for i in range(topN):
        if(ranks[i] < nTPs):
            posRecalls.append(1)
        else:
            posRecalls.append(0)

    curSum = posRecalls[0]
    prevRecall = round(posRecalls[0] / nTPs, 4)
    prevPrec = round(posRecalls[0], 4)
    for i in range(1, topN):
        curSum += posRecalls[i]
        recall = round(curSum / nTPs, 4)
        #precision @k =TP/k
        prec   = round(curSum / (i+1), 4)
        cumPRAUC += ((recall - prevRecall) * (prevPrec + prec) / 2)
        prevRecall = recall
        prevPrec = prec

    cumPRAUC = round(cumPRAUC, 4)
    return cumPRAUC



#python ./weightedRandomforest.py ./input-15 weightedRandomforest 1（个人注释）
#运行时传入四个参数，sys.argv[1]是对输入文件进行，rs是指运行次数（每种方法都使用不同的随机种子集运行10次。））
#outdir指输出结果的策略 nfolds指的是5倍交叉验证。
#将数据分成5个箱子，并在箱子中保留相同百分比的正样本和未标记样本，每次旋转以其中一个作为测试集，而其他四个箱子作为训练集。我们对正未标记学习算法训练5次，在整个数据集中找到可靠的负样本，然后我们训练一个具有可靠的负样本和正标记样本的新分类器，用于预测未标记样本。
#topN = 500，为了评估一种方法的性能，只使用顶部的k个预测，动机是评估该方法在特定疾病的前k个预测中恢复正相关的能力。
#nTrees = 30 每个森林有30颗树，也就是n_estimators的个数
inpath = sys.argv[1]
outdir = sys.argv[2]
rs = int(sys.argv[3])

nfolds = 5
topN = 500
T = 30
nTrees = 30
#mFeats = [4, 5, 6, 7, 8, 9]
mFeats =  [2, 3, 4, 5, 6, 7, 8, 10]
R = np.power(2.0, [5, 6, 7, 8, 9, 10, 11, 12])
#R = np.power(10.0, range(2, 7))

nC = len(mFeats)
nR = len(R)
dFeatures = [f for f in listdir(inpath) if isfile(join(inpath, f))]
for df in dFeatures:
    print("Processing %s" %(df) )
    dId = df.split('_')[0]
    pf = "/".join((inpath, df)) #processing input file: df

    outfile = ".".join((dId, "txt"))
    of = "/".join((outdir, outfile))
    
#    featImp = ".".join((dId, "impFeat")) #feature importance
#    fif = "/".join((outdir, featImp))
# 对输入的csv文件进行操作（个人注释）
    eRecalls = np.zeros(nfolds)
    ePrecisions = np.zeros(nfolds)
    ePRAUCs = np.zeros(nfolds)
    d = np.loadtxt(pf, delimiter = ',')
    p = d[d[:, 5] == 1, :]
    u = d[d[:, 5] == 0, :]
    X_p = p[:, 0:5]
    y_p = p[:, 5]
    X_u = u[:, 0:5]
    y_u = u[:, 5]
    y_u = np.ones(y_u.shape[0]) * -1        
    #nfolds to evaluate the performance
    ikf = 0
    kf = KFold(n_splits = nfolds, shuffle = True, random_state = rs)
    X_p_splits = list(kf.split(X_p))
    X_u_splits = list(kf.split(X_u))
    for ikf in range(nfolds):
        p_train_index, p_test_index = X_p_splits[ikf]
        u_train_index, u_test_index = X_u_splits[ikf]
        X_p_train = X_p[p_train_index]
        y_p_train = y_p[p_train_index]
        X_p_test  = X_p[p_test_index]
        y_p_test  = y_p[p_test_index]
        
        X_u_train= X_u[u_train_index]
        y_u_train= y_u[u_train_index]
        X_u_test = X_u[u_test_index]
        y_u_test = y_u[u_test_index]
#        print("Train:", X_p_train.shape, "test:", X_p_test.shape)
    
        start_time = time.time()
        cvMeans = np.zeros( nC * nR )
        cvStds = np.zeros(nC * nR )
        ithPair = 0
        #nested nfolds to select optimal parameters
        kf2 = KFold(n_splits = nfolds, shuffle = True, random_state = rs)
        for mf in mFeats:
            for r in R:
                recalls = np.zeros(nfolds) #recall rate per each c-r pair
                X_p_cv_splits = list(kf2.split(X_p_train))
                X_u_cv_splits = list(kf2.split(X_u_train))
                for ikf2 in range(nfolds):
                    p_train_cv_index, p_val_cv_index = X_p_cv_splits[ikf2]
                    u_train_cv_index, u_val_cv_index = X_u_cv_splits[ikf2]
                    X_p_cv_train = X_p_train[p_train_cv_index]
                    y_p_cv_train = y_p_train[p_train_cv_index]
                    X_p_cv_val   = X_p_train[p_val_cv_index]
                    y_p_cv_val   = y_p_train[p_val_cv_index]
                
                    X_u_cv_train = X_u_train[u_train_cv_index]
                    y_u_cv_train = y_u_train[u_train_cv_index]
                    X_u_cv_val = X_u_train[u_val_cv_index]
                    y_u_cv_val = y_u_train[u_val_cv_index]
                    
                    #mix validation + unlabel for transductive learning to see how it perform on validation set
                    X_pu_cv_train = np.concatenate((X_p_cv_train, X_u_cv_train), axis = 0)
                    y_pu_cv_train = np.concatenate((y_p_cv_train, y_u_cv_train), axis = 0)
                    X_pu_cv_val = np.concatenate((X_p_cv_val, X_u_cv_val), axis = 0)
                    y_pu_cv_val = np.concatenate((y_p_cv_val,  y_u_cv_val), axis = 0)
                    scaler = StandardScaler().fit(X_pu_cv_train)
                    X_pu_cv_train_transformed = scaler.transform(X_pu_cv_train)
#                    pca = PCA(0.99, svd_solver="full", random_state = 0)
#                    pca.fit(X_pu_cv_train_transformed)
#                    X_pu_cv_train_transformed = pca.transform(X_pu_cv_train_transformed)
                    X_pu_cv_val_transformed = scaler.transform(X_pu_cv_val)
#                    X_pu_cv_val_transformed = pca.transform(X_pu_cv_val_transformed)
# oob_score = False 不使用袋外样品进行估算
                    clf = RandomForestClassifier(n_estimators = nTrees, max_depth = mf, oob_score = False, class_weight = {-1: 1, 1: r}, random_state = 1) 
                    #clf = RandomForestClassifier(n_estimators = nTrees, max_features = mf, oob_score = False, class_weight = 'balanced', random_state = 1)
                    clf.fit(X_pu_cv_train_transformed, y_pu_cv_train)
                    scores = clf.predict_proba(X_pu_cv_val_transformed)[:, 1]
                    orderScores = np.argsort(-scores)
                    topNIndex = orderScores[:topN]
                    #print("avgScores:", avgScores[topNIndex])
                    truePosIndex = np.array(range(y_p_cv_val.shape[0]) )
                    truePosRecall = np.intersect1d(topNIndex, truePosIndex, assume_unique=True)
                    recall = truePosRecall.shape[0] / truePosIndex.shape[0]
#                    recall = calPRAUC(orderScores, y_p_cv_val.shape[0], topN)
                    recalls[ikf2] = recall
                    #scores = clf.predict(X_pu_cv_val_transformed)
                    #nPos = np.sum(scores == 1)
                    #if nPos == 0:
                    #    nPos = 1
                    #posRate = nPos / y_pu_cv_val.shape[0]
                    #recall = recall_score(y_pu_cv_val, scores)
                    #recalls[ikf2] = recall * recall / posRate
                    #print("For mf: %d, fold:%d, recall:%f, F' measure: %f " %(mf, ikf2, recall, recalls[ikf2]))
                    #print(confusion_matrix(y_pu_cv_val, scores))
                avgRecall = np.mean(recalls)
                cvMeans[ithPair] = avgRecall
                stdRecall = np.std(recalls)
                cvStds[ithPair] = stdRecall
#                print("For mfeatures: %d, class_weight ratio: %f, rank of top %d: average recall: %.2f%%, std of recall: %.2f" %(mf, r, topN, avgRecall*100, stdRecall ))
#                print("For each fold:", recalls)
                ithPair += 1
        elapsed_time = time.time() - start_time
        cvMaxMeanIndex = np.argmax(cvMeans)
        optimalM = mFeats[cvMaxMeanIndex // nR]
        optimalR = R[cvMaxMeanIndex % nR]
 #       print("cv-MaxMean:", cvMeans[cvMaxMeanIndex], "cv-MaxMean_std:", cvStds[cvMaxMeanIndex], "cvMaxMeanIndex:", cvMaxMeanIndex)
        print("disease:", dId, ", randomSeed:", rs, ", ithFold:", ikf, ", optimalM:", optimalM, ", optimalR:", optimalR, ", cv-MaxMean:", cvMeans[cvMaxMeanIndex])
#        print("cross-validation time elapsed: %.2f" %(elapsed_time) )
        #After parameter selection, we evaluate on the test set with the optimal parameters
        X_test = np.concatenate((X_p_test, X_u_test), axis = 0)
        y_test = np.concatenate((y_p_test, y_u_test), axis = 0)
        X_train = np.concatenate((X_p_train, X_u_train), axis = 0)
        y_train = np.concatenate((y_p_train, y_u_train), axis = 0)
        scaler = StandardScaler().fit(X_train)
        X_train_transformed = scaler.transform(X_train)
#        pca = PCA(0.99, svd_solver="full", random_state = 0)
#        pca.fit(X_train_transformed)
#        X_train_transformed = pca.transform(X_train_transformed)
        X_test_transformed = scaler.transform(X_test)
#        X_test_transformed = pca.transform(X_test_transformed)
        clf = RandomForestClassifier(n_estimators = nTrees, max_depth = optimalM, oob_score = False, class_weight = {-1: 1, 1: optimalR}, random_state = 1)
        #clf = RandomForestClassifier(n_estimators = nTrees, max_features = optimalM, oob_score = False, class_weight = 'balanced', random_state = 1)
        clf.fit(X_train_transformed, y_train)
        #scores = clf.predict(X_test_transformed)
        scores = clf.predict_proba(X_test_transformed)[:, 1]
#        scoreList = [str(item) for item in scores]
#        scoreStr = ','.join(scoreList)
        #recall = recall_score(y_test, scores)
        orderScores = np.argsort(-scores)
        orderList = [str(item) for item in orderScores]
        orderStr = ','.join(orderList)
        topNIndex = orderScores[:topN]
        #print("avgScores:", avgScores[topNIndex])
        truePosIndex = np.array(range(y_p_test.shape[0]) )
        truePosRecall = np.intersect1d(topNIndex, truePosIndex, assume_unique=True)
        recall = truePosRecall.shape[0] / truePosIndex.shape[0]
        precision = truePosRecall.shape[0] / topN
        prauc = calPRAUC(orderScores, y_p_test.shape[0], topN)
        eRecalls[ikf] = recall
        ePrecisions[ikf] = precision
        ePRAUCs[ikf] = prauc

        print("dId: %s, randomState: %d, %dth-fold, recall: %.2f%%, precision: %.2f%%, prauc: %.4f" %(dId, rs, ikf, recall*100, precision*100, prauc))
        with open(of, "a") as output:
            output.write("%s-RandomState%d-%dth fold, number of true positive:%d\n" %(dId, rs, ikf, y_p_test.shape[0]))   
            output.write("%s\n" %(orderStr))
            output.write("END\n")
    mRecall = np.mean(eRecalls)
    stdRecall = np.std(eRecalls)
    mPrec   = np.mean(ePrecisions)
    stdPrec = np.std(ePrecisions)
    mPRAUC = np.mean(ePRAUCs)
    stdPRAUC = np.std(ePRAUCs)
    recallList = [str(item) for item in eRecalls]
    precList   = [str(item) for item in ePrecisions]
    praucList  = [str(item) for item in ePRAUCs]
    recallStr = ','.join(recallList)
    precStr = ','.join(precList)
    praucStr = ','.join(praucList)
    with open (of, "a") as output:
        output.write("%s-RandomState%d, mean+-std recall:%.4f,%.4f\n" %(dId, rs, mRecall, stdRecall))
        output.write("%s-RandomState%d, mean+-std precision:%.4f,%.4f\n" %(dId, rs, mPrec, stdPrec))
        output.write("%s-RandomState%d, mean+-std prauc:%.4f,%.4f\n" %(dId, rs, mPRAUC, stdPRAUC))
        output.write("%s-RandomState%d, 5-fold cv recall:%s\n" %(dId, rs, recallStr))
        output.write("%s-RandomState%d, 5-fold cv precision:%s\n" %(dId, rs, precStr))
        output.write("%s-RandomState%d, 5-fold cv prauc:%s\n" %(dId, rs, praucStr))
        output.write("END\n")
    print("summary of %s, randomSeed: %d, top %d, mean/std of prauc, mean/std of recall, mean/std of precision: %f,%f,%f,%f,%f,%f" %(dId, rs, topN, mPRAUC, stdPRAUC, mRecall, stdRecall, mPrec, stdPrec))
    print(eRecalls)
    print(ePrecisions)
    print(ePRAUCs)
    print("END")

~~~

## 6.python提取7685个序列

~~~python
# 导入所需的包
import numpy as np

np.set_printoptions(threshold=np.inf)
# 导入npy文件路径位置
test1 = np.load(r'C:\Users\12569\Desktop\有用数据\fold\0_x_train_1.npy')
for i1 in range(0, 7684):
    for j in range(0, 32):
        a = [i for i, x in enumerate(test1[i1][j - 1]) if x == 1]
        if (a == [1]):
            print("T", end='')
        elif (a == [0]):
            print("A", end='')
        elif (a == [2]):
            print("C", end='')
        elif (a == [3]):
            print("G", end='')
    print("")
# print(test1)


#
# import numpy as np
# b = np.ones((2,2,2))
# with open(r'C:\Users\12569\Desktop\有用数据\fold\0_x_train_1.npy','w')as outfile:
#     for slice_2d in b:
#         np.savetxt(outfile, slice_2d, fmt ='%.3f', delimiter =',')
# c = np.loadtxt('log.txt', delimiter =',').reshape((2,2,2))

~~~

## 7.python提取7685条序列与哪一种疾病的关系

~~~python
# 导入所需的包
import numpy as np

np.set_printoptions(threshold=np.inf)
# 导入npy文件路径位置
test1 = np.load(r'C:\Users\12569\Desktop\有用数据\fold\0_x_train_2.npy')
for i in range(0, 7685):
    for j in range(0, 21):
        if test1[i][j]== 1:
            print(i+1,j+1)
#print(test1)


#
# import numpy as np
# b = np.ones((2,2,2))
# with open(r'C:\Users\12569\Desktop\有用数据\fold\0_x_train_1.npy','w')as outfile:
#     for slice_2d in b:
#         np.savetxt(outfile, slice_2d, fmt ='%.3f', delimiter =',')
# c = np.loadtxt('log.txt', delimiter =',').reshape((2,2,2))

~~~

## 8.21种疾病piRNA进行相关性赋值

~~~c++
#include<iostream>
using namespace std;
bool flag=false;
int i=-1;
struct piRNA{
    int pi,dis;
};
piRNA a[4350];
int main(){

    freopen("in1.txt","r",stdin);
    freopen("out.txt","w",stdout);
    int c,d,sum=0;
    while(cin>>c>>d){
        if(d==1){
            sum++;
            cout<<d<<endl;

        }
        else{
            cout<<"0"<<endl;
        }
    }
    cout<<sum;
    return 0;
}


~~~

## 9.提取可靠负集代码

~~~python
from __future__ import division, print_function
import numpy as np
from sklearn.datasets import make_moons
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import precision_recall_curve
import pandas as pd


np.set_printoptions(threshold=np.inf)


import re
from sklearn.preprocessing import LabelEncoder
#from pulearn import BaggingPuClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
# 读取xls（绝对路径）
# 读取xlsx（第二个sheet）（设置sheet位置）
# 选择部分列读取（字符列表形式）
#C:\Users\12569\Desktop\构建可靠负集
df_P = pd.read_excel(io=r'C:\Users\12569\Desktop\构建可靠负集\正集数据.xls', sheet_name=0, usecols=['name', 'Sequence'],
                     nrows=211)
df_U = pd.read_excel(io=r'C:\Users\12569\Desktop\构建可靠负集\未标记数据集.xls', sheet_name=0, usecols=['name', 'Sequence'],
                     nrows=4141)

data_P = np.array(df_P)
data_U = np.array(df_U)


# 将独热编码统一为成numpy数组
def handle_array(data):
    length = len(data)
    data_X = np.zeros([length, 32 * 4])
    for i in range(length):
        data_X[i][:] = (data[i][1].reshape(1, -1))[0][0:128]
    return data_X


# 将碱基数统一为32个  超出的碱基去除，不够的补0
def handle_length(my_array):
    length = len(my_array)
    if length >= 32:
        return my_array[:32]
    while length < 32:
        my_array = np.append(my_array, [[0, 0, 0, 0]], axis=0)  # 添加整行元素
        length += 1
    return my_array


# 处理头字符
def head_str(ch):
    if ch == 'A':
        return [[1, 0, 0, 0]]
    elif ch == 'C':
        return [[0, 1, 0, 0]]
    elif ch == 'G':
        return [[0, 0, 1, 0]]
    elif ch == 'T':
        return [[0, 0, 0, 1]]


# 返回一个 L x 4 的 numpy 数组
def one_hot_encoder(my_str):
    onehot_encoded = head_str(my_str[0])
    str = my_str[1:]
    for ch in str:
        if ch == 'A':
            onehot_encoded = np.append(onehot_encoded, [[1, 0, 0, 0]], axis=0)
        elif ch == 'C':
            onehot_encoded = np.append(onehot_encoded, [[0, 1, 0, 0]], axis=0)
        elif ch == 'G':
            onehot_encoded = np.append(onehot_encoded, [[0, 0, 1, 0]], axis=0)
        elif ch == 'T':
            onehot_encoded = np.append(onehot_encoded, [[0, 0, 0, 1]], axis=0)
    return onehot_encoded


# 对正集进行编码
for i in range(len(data_P)):
    data_P[i, 1] = handle_length(one_hot_encoder(data_P[i, 1]))

# 对未知集进行编码
for i in range(len(data_U)):
    data_U[i, 1] = handle_length(one_hot_encoder(data_U[i, 1]))

data_P = handle_array(data_P)
data_U = handle_array(data_U)

NP = data_P.shape[0]
NU = data_U.shape[0]

T = 1000
K = NP
train_label = np.zeros(shape=(NP + K,))
train_label[:NP] = 1.0
n_oob = np.zeros(shape=(NU,))
f_oob = np.zeros(shape=(NU, 2))
for i in range(T):
    # Bootstrap resample
    bootstrap_sample = np.random.choice(np.arange(NU), replace=True, size=K)
    # Positive set + bootstrapped unlabeled set
    data_bootstrap = np.concatenate((data_P, data_U[bootstrap_sample, :]), axis=0)
    # Train model
    model = DecisionTreeClassifier(max_depth=None, max_features=None,
                                   criterion='gini', class_weight='balanced')
    model.fit(data_bootstrap, train_label)
    # Index for the out of the bag (oob) samples
    idx_oob = sorted(set(range(NU)) - set(np.unique(bootstrap_sample)))
    # Transductive learning of oob samples
    f_oob[idx_oob] += model.predict_proba(data_U[idx_oob])
    n_oob[idx_oob] += 1
predict_proba = f_oob[:, 1] / n_oob

print(predict_proba)

# 设置中文和负号正常显示
plt.rcParams['font.sans-serif'] = 'Microsoft YaHei'
plt.rcParams['axes.unicode_minus'] = False

# 设置图形的显示风格
plt.style.use('ggplot')
# 绘图：daily_Ionset_r_c1_predicted的箱线图


plt.boxplot(x=predict_proba,  # 指定绘图数据

            patch_artist=True,  # 要求用自定义颜色填充盒形图，默认白色填充

            showmeans=True,  # 以点的形式显示均值

            boxprops={'color': 'black', 'facecolor': '#9999ff'},  # 设置箱体属性，填充色和边框色

            flierprops={'marker': 'o', 'markerfacecolor': 'red', 'color': 'black'},  # 设置异常值属性，点的形状、填充色和边框色

            meanprops={'marker': 'D', 'markerfacecolor': 'indianred'},  # 设置均值点的属性，点的形状、填充色

            medianprops={'linestyle': '--', 'color': 'orange'})  # 设置中位数线的属性，线的类型和颜色

# 设置y轴的范围
# plt.ylim(0,3000)

# 去除箱线图的上边框与右边框的刻度标签
plt.tick_params(top='off', right='off')
# 保存图像
plt.savefig('./results_imgs.png', bbox_inches='tight')  # 保存的文件名，裁剪掉图表多余的空白区域
# 显示图形
plt.show()

# 获取前210个得分较低的元素下标(即数据所在行)
k = 211
ind = np.argpartition(predict_proba, k)[:k]
print(ind)
# 读取未标记数据
p = r'C:\Users\12569\Desktop\构建可靠负集\未标记数据集.csv'
with open(p, encoding='utf-8') as f:
    data = np.loadtxt(f, str, delimiter=",", skiprows=1) #skiprows是跳过第1行

# 提取可靠负集数据
data_N = data[ind]
# 插入表头
title = np.array(['name', 'Sequence'])
data_N = np.insert(data_N, 0, title, axis=0)
# 将文件写入csv文件
np.savetxt(r'C:\Users\12569\Desktop\可靠负集数据5000-0511.csv', data_N, fmt='%s', delimiter=',')

# Plot the class probabilities for the unlabeled samples
fig = plt.figure(figsize=(50, 8))
ax1 = fig.add_subplot(1, 2, 1)

arr = np.arange(4140)
# 打乱numpy数组的组内顺序
np.random.shuffle(arr)

sp = ax1.scatter(arr, predict_proba, c=predict_proba,
                 linewidth=0, s=5, alpha=0.5, cmap=plt.cm.plasma, label='unlabeled')
#plt.grid()

plt.colorbar(sp, label='Class probability on Unlabeled set')
plt.show()
#plt.grid()
#plt.show()
~~~

