# 分类代码

~~~python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import time
import numpy as np
from numpy import linalg as LA
from sklearn.model_selection import train_test_split
from sklearn.model_selection import ShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import confusion_matrix

from scipy import stats

import sys
from os import listdir
from os.path import isfile, join

# 判断是否输入了4个参数（个人注释）
# ./weightedRandomforest.py  ./input-15  weightedRandomforest  5



if (len(sys.argv) != 4):
    sys.exit("Usage: %s directory-for-input-files outdir randomSeed\n" % (sys.argv[0]))


# 参数计算，定义calPRAUC函数，计算查准率，查全率，PR曲线（个人注释）

#precision 所有预测为真的里面包括多少个实际真的  预测了100个真的 里面只有30个真的预测为真的，30%

#recall 所有实际真的里面将正类预测为真的      实际有100个真的  将其中99个真的预测为真的 99%

#求auc曲线下梯形的面积，也就是看该训练模型的区分能力

# 用PRAUC是因为查准率和查全率都集中分析正样本


 # prauc = calPRAUC(orderScores, y_p_test.shape[0], topN)  其中topN=500 ntp所有正样本被测为负的概率
def calPRAUC(ranks, nTPs, topN):
    cumPRAUC = 0
    posRecalls = list()
    for i in range(topN):
        if (ranks[i] < nTPs):
            posRecalls.append(1)
        else:
            posRecalls.append(0)

    curSum = posRecalls[0]
    prevRecall = round(posRecalls[0] / nTPs, 4)
    prevPrec = round(posRecalls[0], 4)
    for i in range(1, topN):
        curSum += posRecalls[i]
        recall = round(curSum / nTPs, 4)
        # precision @k =TP/k
        prec = round(curSum / (i + 1), 4)
        cumPRAUC += ((recall - prevRecall) * (prevPrec + prec) / 2)
        prevRecall = recall
        prevPrec = prec

    cumPRAUC = round(cumPRAUC, 4)
    return cumPRAUC








# python ./weightedRandomforest.py ./input-15 weightedRandomforest 1（个人注释）
# 运行时传入四个参数，sys.argv[1]是对输入文件进行，rs是指运行次数（每种方法都使用不同的随机种子集运行10次。））
# outdir指输出结果的策略 nfolds指的是5倍交叉验证。
# 将数据分成5个箱子，并在箱子中保留相同百分比的正样本和未标记样本，每次旋转以其中一个作为测试集，而其他四个箱子作为训练集。我们对正未标记学习算法训练5次，在整个数据集中找到可靠的负样本，然后我们训练一个具有可靠的负样本和正标记样本的新分类器，用于预测未标记样本。
# topN = 500，为了评估一种方法的性能，只使用顶部的k个预测，动机是评估该方法在特定疾病的前k个预测中恢复正相关的能力。
# nTrees = 30 每个森林有30颗树，也就是n_estimators的个数
inpath = sys.argv[1] #输入路径
outdir = sys.argv[2] #输出目录
rs = int(sys.argv[3])  #运行随机种子编号

nfolds = 5
topN = 500
T = 30
nTrees = 30
# mFeats = [4, 5, 6, 7, 8, 9]
mFeats = [2, 3, 4, 5, 6, 7, 8, 10]
R = np.power(2.0, [5, 6, 7, 8, 9, 10, 11, 12])  #求2的n次方 求最优参数
# R = np.power(10.0, range(2, 7))

nC = len(mFeats)
nR = len(R)
dFeatures = [f for f in listdir(inpath) if isfile(join(inpath, f))]
for df in dFeatures:
    print("Processing %s" % (df))
    dId = df.split('_')[0]
    pf = "/".join((inpath, df))  # ./input-15/D001_features.csv

    outfile = ".".join((dId, "txt")) #D001.txt
    of = "/".join((outdir, outfile))    #weightRandomforest/D001.txt

    #    featImp = ".".join((dId, "impFeat")) #feature importance
    #    fif = "/".join((outdir, featImp))
    # 对输入的csv文件进行操作（个人注释）
    eRecalls = np.zeros(nfolds) #返回5个全0的数
    ePrecisions = np.zeros(nfolds)   #返回5个全0的数
    ePRAUCs = np.zeros(nfolds) #返回5个全0的数
    d = np.loadtxt(pf, delimiter=',') #对pf是路径  csv文件进行输出
    #注意：X[:, m:n]即取矩阵X的所有行中的的第m到n-1列数据，含左不含右
    tt=6  #不去 是6       去1是5      去2是4       去3是3       去4是2
    xss=7
    p = d[d[:, tt] == 1, :]  #所有行第6列元素等于1
    u = d[d[:, tt] == 0, :]  #所有行第6列元素等于0的行
    X_p = p[:, 0:tt]      # X_p正样本0-6列
    y_p = p[:, tt]         # y_p正样本第7列
    X_u = u[:, 0:tt]       #未标记样本0-6列
    y_u = u[:, tt]          #未标记样本第7列
    xx = d[:,xss]    #专门第7列
    y_u = np.ones(y_u.shape[0]) * -1  #将y_u中未标记样本的值0都赋值为-1
    # nfolds to evaluate the performance
    # ikf是第几倍交叉验证
    ikf = 0
    #shuffle,这其实在python里面也是一个函数，就是打乱顺序的意思。
    # 那么它的作用是什么呢？比如说我们现在是将索引从0-19的20个索引利用K折验证方法，也就是说要将数据分成两块。
    # 如果按照default shuffle = False，
    # 那么分法就是按照顺序分，也就是第一个的验证集是0-9，第二个是10-19。如果shuffle = True,那么则表示将打乱顺序后再进行分配。比如我对n_splits = 2 时候根据shuffle 的boolean值不同进行了测试
    #随机种子是保持不同的结果
    kf = KFold(n_splits=nfolds, shuffle=False, random_state=rs)
    X_p_splits = list(kf.split(X_p)) #将正样本分为训练集和测试集
    X_u_splits = list(kf.split(X_u)) #将未标记样本分为训练集和测试集


    #自己----------------------------------------------------------------------------------修改部分
    allpositive = p[:, xss]  # 序号
    allunknown = u[:, xss]  # 序号
    # print(allpositive[50])
    sum_positive = 0
    sum_unknown = 0






    for ikf in range(nfolds):
        p_train_index, p_test_index = X_p_splits[ikf]  #测试集是0
        u_train_index, u_test_index = X_u_splits[ikf]
        X_p_train = X_p[p_train_index]
        y_p_train = y_p[p_train_index]
        X_p_test = X_p[p_test_index]
        y_p_test = y_p[p_test_index]

        X_u_train = X_u[u_train_index]
        y_u_train = y_u[u_train_index]
        X_u_test = X_u[u_test_index]
        y_u_test = y_u[u_test_index]
        #print("Train:", X_p_train.shape, "test:", X_p_test.shape)

        start_time = time.time()
        cvMeans = np.zeros(nC * nR)
        cvStds = np.zeros(nC * nR)
        ithPair = 0
        # nested nfolds to select optimal parameters
        kf2 = KFold(n_splits=nfolds, shuffle=False, random_state=rs)
        for mf in mFeats:
            for r in R:
                recalls = np.zeros(nfolds)  # recall rate per each c-r pair
                X_p_cv_splits = list(kf2.split(X_p_train))
                X_u_cv_splits = list(kf2.split(X_u_train))
                for ikf2 in range(nfolds):
                    p_train_cv_index, p_val_cv_index = X_p_cv_splits[ikf2]
                    u_train_cv_index, u_val_cv_index = X_u_cv_splits[ikf2]
                    X_p_cv_train = X_p_train[p_train_cv_index]
                    y_p_cv_train = y_p_train[p_train_cv_index]
                    X_p_cv_val = X_p_train[p_val_cv_index]
                    y_p_cv_val = y_p_train[p_val_cv_index]

                    X_u_cv_train = X_u_train[u_train_cv_index]
                    y_u_cv_train = y_u_train[u_train_cv_index]
                    X_u_cv_val = X_u_train[u_val_cv_index]
                    y_u_cv_val = y_u_train[u_val_cv_index]

                    # mix validation + unlabel for transductive learning to see how it perform on validation set
                    X_pu_cv_train = np.concatenate((X_p_cv_train, X_u_cv_train), axis=0)
                    y_pu_cv_train = np.concatenate((y_p_cv_train, y_u_cv_train), axis=0)
                    X_pu_cv_val = np.concatenate((X_p_cv_val, X_u_cv_val), axis=0)
                    y_pu_cv_val = np.concatenate((y_p_cv_val, y_u_cv_val), axis=0)
                    scaler = StandardScaler().fit(X_pu_cv_train)
                    X_pu_cv_train_transformed = scaler.transform(X_pu_cv_train)
                    #                    pca = PCA(0.99, svd_solver="full", random_state = 0)
                    #                    pca.fit(X_pu_cv_train_transformed)
                    #                    X_pu_cv_train_transformed = pca.transform(X_pu_cv_train_transformed)
                    X_pu_cv_val_transformed = scaler.transform(X_pu_cv_val)
                    #                    X_pu_cv_val_transformed = pca.transform(X_pu_cv_val_transformed)
                    # oob_score = False 不使用袋外样品进行估算
                    clf = RandomForestClassifier(n_estimators=nTrees, max_depth=mf, oob_score=False,
                                                 class_weight={-1: 1, 1: r}, random_state=1)
                    # clf = RandomForestClassifier(n_estimators = nTrees, max_features = mf, oob_score = False, class_weight = 'balanced', random_state = 1)
                    clf.fit(X_pu_cv_train_transformed, y_pu_cv_train)
                    #分数 取第2列，即分类为1的序列
                    scores = clf.predict_proba(X_pu_cv_val_transformed)[:, 1]
                    # -scores从小到大排列，即越靠前的预测越准确
                    orderScores = np.argsort(-scores)
                    topNIndex = orderScores[:topN]
                    # print("avgScores:", avgScores[topNIndex])
                    truePosIndex = np.array(range(y_p_cv_val.shape[0]))
                    truePosRecall = np.intersect1d(topNIndex, truePosIndex, assume_unique=True)
                    recall = truePosRecall.shape[0] / truePosIndex.shape[0]
                    #                    recall = calPRAUC(orderScores, y_p_cv_val.shape[0], topN)
                    recalls[ikf2] = recall
                    # scores = clf.predict(X_pu_cv_val_transformed)
                    # nPos = np.sum(scores == 1)
                    # if nPos == 0:
                    #    nPos = 1
                    # posRate = nPos / y_pu_cv_val.shape[0]
                    # recall = recall_score(y_pu_cv_val, scores)
                    # recalls[ikf2] = recall * recall / posRate
                    # print("For mf: %d, fold:%d, recall:%f, F' measure: %f " %(mf, ikf2, recall, recalls[ikf2]))
                    # print(confusion_matrix(y_pu_cv_val, scores))
                avgRecall = np.mean(recalls)
                cvMeans[ithPair] = avgRecall
                stdRecall = np.std(recalls)
                cvStds[ithPair] = stdRecall
                #                print("For mfeatures: %d, class_weight ratio: %f, rank of top %d: average recall: %.2f%%, std of recall: %.2f" %(mf, r, topN, avgRecall*100, stdRecall ))
                #                print("For each fold:", recalls)
                ithPair += 1
        elapsed_time = time.time() - start_time
        cvMaxMeanIndex = np.argmax(cvMeans)
        optimalM = mFeats[cvMaxMeanIndex // nR]
        optimalR = R[cvMaxMeanIndex % nR]
        #       print("cv-MaxMean:", cvMeans[cvMaxMeanIndex], "cv-MaxMean_std:", cvStds[cvMaxMeanIndex], "cvMaxMeanIndex:", cvMaxMeanIndex)
        print("disease:", dId, ", randomSeed:", rs, ", ithFold:", ikf, ", optimalM:", optimalM, ", optimalR:", optimalR,
              ", cv-MaxMean:", cvMeans[cvMaxMeanIndex])
        #        print("cross-validation time elapsed: %.2f" %(elapsed_time) )
        # After parameter selection, we evaluate on the test set with the optimal parameters
        X_test = np.concatenate((X_p_test, X_u_test), axis=0)    #拼接函数 axis等于0为纵向拼接
        y_test = np.concatenate((y_p_test, y_u_test), axis=0)  #y是指类别
        X_train = np.concatenate((X_p_train, X_u_train), axis=0)
        y_train = np.concatenate((y_p_train, y_u_train), axis=0)
        scaler = StandardScaler().fit(X_train)
        X_train_transformed = scaler.transform(X_train)
        #        pca = PCA(0.99, svd_solver="full", random_state = 0)
        #        pca.fit(X_train_transformed)
        #        X_train_transformed = pca.transform(X_train_transformed)
        X_test_transformed = scaler.transform(X_test)
        #        X_test_transformed = pca.transform(X_test_transformed)
        clf = RandomForestClassifier(n_estimators=nTrees, max_depth=optimalM, oob_score=False,
                                     class_weight={-1: 1, 1: optimalR}, random_state=1) #-1占比1 然后optimalR占比32
        # clf = RandomForestClassifier(n_estimators = nTrees, max_features = optimalM, oob_score = False, class_weight = 'balanced', random_state = 1)
        clf.fit(X_train_transformed, y_train) #训练数据
        # scores = clf.predict(X_test_transformed)
        #输出第2列所有为真的那列
        #print(X_test_transformed)
        scores = clf.predict_proba(X_test_transformed)[:, 1]
       # print(scores)
        #        scoreList = [str(item) for item in scores]
        #        scoreStr = ','.join(scoreList)
        # recall = recall_score(y_test, scores)
        orderScores = np.argsort(-scores)

        # -------------------------------------------------------------------------------------------------------------
        #print(orderScores)
        ouput_scores=[]
        for i in orderScores:  # 24 22
            if i < len(p_test_index):
                str0 = str(allpositive[i + sum_positive])
                str1 = str0 + "\t" + str(round(scores[i], 6))


            else:
                str0 = str(allunknown[i - len(p_test_index) + sum_unknown])
                str1 = str0 + "\t" + str(round(scores[i], 6))
            ouput_scores.append(str1)
        #print(ouput_scores)
        orderStr = ','.join(ouput_scores)
        sum_positive += len(p_test_index)
        sum_unknown += len(u_test_index)

        #-------------------------------------------------------------------------------------------------------------


        #orderList = [str([xx[item+ikf*870]]) for item in orderScores]
        # orderStr = ','.join(orderList)

        topNIndex = orderScores[:topN]
        #print("avgScores:", avgScores[topNIndex])
        #42等于测试集中行数
        truePosIndex = np.array(range(y_p_test.shape[0]))  #测试集的行数
        #topNIndex和真正的交集筛掉truePosINdex
        # TP是所有预测为正的-假的预测为正的  故truePosIndex为假的预测为正的
        # a = np.array([8, 2, 3, 4, 2, 4, 1])
        # b = np.array([7, 9, 5, 6, 3])
        # c = np.setdiff1d(a, b, True)
        # print(c)  # [8 2 4 2 4 1]
        #即truePosIndex为假的预测为正的
        # topNIndex为全部预测为正的
        truePosRecall = np.intersect1d(topNIndex, truePosIndex, assume_unique=True)
        # 42/42
        # TP/(TP+FN)  即truePosRecall.shape[0] 为TP
        recall = truePosRecall.shape[0] / truePosIndex.shape[0]
        #truePosRecall是真实的五分之一
        precision = truePosRecall.shape[0] / topN #真实的42/500
        prauc = calPRAUC(orderScores, y_p_test.shape[0], topN)
        eRecalls[ikf] = recall
        ePrecisions[ikf] = precision
        ePRAUCs[ikf] = prauc
        ssddasdsf="\n".join(ouput_scores)
        print("dId: %s, randomState: %d, %dth-fold, recall: %.2f%%, precision: %.2f%%, prauc: %.4f" % (
        dId, rs, ikf, recall * 100, precision * 100, prauc))
        with open(of, "a") as output:
            output.write("%s-RandomState%d-%dth fold, number of true positive:%d\n" % (dId, rs, ikf, y_p_test.shape[0]))
            output.write("%s\n" % (ssddasdsf))
            output.write("END\n")
    mRecall = np.mean(eRecalls)
    stdRecall = np.std(eRecalls)
    mPrec = np.mean(ePrecisions)
    stdPrec = np.std(ePrecisions)
    mPRAUC = np.mean(ePRAUCs)
    stdPRAUC = np.std(ePRAUCs)
    recallList = [str(item) for item in eRecalls]
    precList = [str(item) for item in ePrecisions]
    praucList = [str(item) for item in ePRAUCs]
    recallStr = ','.join(recallList)
    precStr = ','.join(precList)
    praucStr = ','.join(praucList)
    with open(of, "a") as output:
        output.write("%s-RandomState%d, mean+-std recall:%.4f,%.4f\n" % (dId, rs, mRecall, stdRecall))
        output.write("%s-RandomState%d, mean+-std precision:%.4f,%.4f\n" % (dId, rs, mPrec, stdPrec))
        output.write("%s-RandomState%d, mean+-std prauc:%.4f,%.4f\n" % (dId, rs, mPRAUC, stdPRAUC))
        output.write("%s-RandomState%d, 5-fold cv recall:%s\n" % (dId, rs, recallStr))
        output.write("%s-RandomState%d, 5-fold cv precision:%s\n" % (dId, rs, precStr))
        output.write("%s-RandomState%d, 5-fold cv prauc:%s\n" % (dId, rs, praucStr))
        output.write("END\n")
    print(
        "summary of %s, randomSeed: %d, top %d, mean/std of prauc, mean/std of recall, mean/std of precision: %f,%f,%f,%f,%f,%f" % (
        dId, rs, topN, mPRAUC, stdPRAUC, mRecall, stdRecall, mPrec, stdPrec))
    print(eRecalls)
    print(ePrecisions)
    print(ePRAUCs)
    print("END")

~~~

# 构建可靠的负集

~~~python
from __future__ import division, print_function
import numpy as np
from sklearn.datasets import make_moons
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import precision_recall_curve
import pandas as pd


np.set_printoptions(threshold=np.inf)


import re
from sklearn.preprocessing import LabelEncoder
#from pulearn import BaggingPuClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
# 读取xls（绝对路径）
# 读取xlsx（第二个sheet）（设置sheet位置）
# 选择部分列读取（字符列表形式）
#C:\Users\12569\Desktop\构建可靠负集
df_P = pd.read_excel(io=r'C:\Users\12569\Desktop\构建可靠负集\正集数据.xls', sheet_name=0, usecols=['name', 'Sequence'],
                     nrows=211)
df_U = pd.read_excel(io=r'C:\Users\12569\Desktop\构建可靠负集\未标记数据集.xls', sheet_name=0, usecols=['name', 'Sequence'],
                     nrows=4141)

data_P = np.array(df_P)
data_U = np.array(df_U)


# 将独热编码统一为成numpy数组
def handle_array(data):
    length = len(data)
    data_X = np.zeros([length, 32 * 4])
    for i in range(length):
        data_X[i][:] = (data[i][1].reshape(1, -1))[0][0:128]
    return data_X


# 将碱基数统一为32个  超出的碱基去除，不够的补0
def handle_length(my_array):
    length = len(my_array)
    if length >= 32:
        return my_array[:32]
    while length < 32:
        my_array = np.append(my_array, [[0, 0, 0, 0]], axis=0)  # 添加整行元素
        length += 1
    return my_array


# 处理头字符
def head_str(ch):
    if ch == 'A':
        return [[1, 0, 0, 0]]
    elif ch == 'C':
        return [[0, 1, 0, 0]]
    elif ch == 'G':
        return [[0, 0, 1, 0]]
    elif ch == 'T':
        return [[0, 0, 0, 1]]


# 返回一个 L x 4 的 numpy 数组
def one_hot_encoder(my_str):
    onehot_encoded = head_str(my_str[0])
    str = my_str[1:]
    for ch in str:
        if ch == 'A':
            onehot_encoded = np.append(onehot_encoded, [[1, 0, 0, 0]], axis=0)
        elif ch == 'C':
            onehot_encoded = np.append(onehot_encoded, [[0, 1, 0, 0]], axis=0)
        elif ch == 'G':
            onehot_encoded = np.append(onehot_encoded, [[0, 0, 1, 0]], axis=0)
        elif ch == 'T':
            onehot_encoded = np.append(onehot_encoded, [[0, 0, 0, 1]], axis=0)
    return onehot_encoded


# 对正集进行编码
for i in range(len(data_P)):
    data_P[i, 1] = handle_length(one_hot_encoder(data_P[i, 1]))

# 对未知集进行编码
for i in range(len(data_U)):
    data_U[i, 1] = handle_length(one_hot_encoder(data_U[i, 1]))

data_P = handle_array(data_P)
data_U = handle_array(data_U)

NP = data_P.shape[0]
NU = data_U.shape[0]

T = 1000
K = NP
train_label = np.zeros(shape=(NP + K,))
train_label[:NP] = 1.0
n_oob = np.zeros(shape=(NU,))
f_oob = np.zeros(shape=(NU, 2))
for i in range(T):
    # Bootstrap resample
    bootstrap_sample = np.random.choice(np.arange(NU), replace=True, size=K)
    # Positive set + bootstrapped unlabeled set
    data_bootstrap = np.concatenate((data_P, data_U[bootstrap_sample, :]), axis=0)
    # Train model
    model = DecisionTreeClassifier(max_depth=None, max_features=None,
                                   criterion='gini', class_weight='balanced')
    model.fit(data_bootstrap, train_label)
    # Index for the out of the bag (oob) samples
    idx_oob = sorted(set(range(NU)) - set(np.unique(bootstrap_sample)))
    # Transductive learning of oob samples
    f_oob[idx_oob] += model.predict_proba(data_U[idx_oob])
    n_oob[idx_oob] += 1
predict_proba = f_oob[:, 1] / n_oob

print(predict_proba)

# 设置中文和负号正常显示
plt.rcParams['font.sans-serif'] = 'Microsoft YaHei'
plt.rcParams['axes.unicode_minus'] = False

# 设置图形的显示风格
plt.style.use('ggplot')
# 绘图：daily_Ionset_r_c1_predicted的箱线图


plt.boxplot(x=predict_proba,  # 指定绘图数据

            patch_artist=True,  # 要求用自定义颜色填充盒形图，默认白色填充

            showmeans=True,  # 以点的形式显示均值

            boxprops={'color': 'black', 'facecolor': '#9999ff'},  # 设置箱体属性，填充色和边框色

            flierprops={'marker': 'o', 'markerfacecolor': 'red', 'color': 'black'},  # 设置异常值属性，点的形状、填充色和边框色

            meanprops={'marker': 'D', 'markerfacecolor': 'indianred'},  # 设置均值点的属性，点的形状、填充色

            medianprops={'linestyle': '--', 'color': 'orange'})  # 设置中位数线的属性，线的类型和颜色

# 设置y轴的范围
# plt.ylim(0,3000)

# 去除箱线图的上边框与右边框的刻度标签
plt.tick_params(top='off', right='off')
# 保存图像
plt.savefig('./results_imgs.png', bbox_inches='tight')  # 保存的文件名，裁剪掉图表多余的空白区域
# 显示图形
plt.show()

# 获取前210个得分较低的元素下标(即数据所在行)
k = 211
ind = np.argpartition(predict_proba, k)[:k]
print(ind)
# 读取未标记数据
p = r'C:\Users\12569\Desktop\构建可靠负集\未标记数据集.csv'
with open(p, encoding='utf-8') as f:
    data = np.loadtxt(f, str, delimiter=",", skiprows=1) #skiprows是跳过第1行

# 提取可靠负集数据
data_N = data[ind]
# 插入表头
title = np.array(['name', 'Sequence'])
data_N = np.insert(data_N, 0, title, axis=0)
# 将文件写入csv文件
np.savetxt(r'C:\Users\12569\Desktop\可靠负集数据5000-0511.csv', data_N, fmt='%s', delimiter=',')

# Plot the class probabilities for the unlabeled samples
fig = plt.figure(figsize=(50, 8))
ax1 = fig.add_subplot(1, 2, 1)

arr = np.arange(4140)
# 打乱numpy数组的组内顺序
np.random.shuffle(arr)

sp = ax1.scatter(arr, predict_proba, c=predict_proba,
                 linewidth=0, s=5, alpha=0.5, cmap=plt.cm.plasma, label='unlabeled')
#plt.grid()

plt.colorbar(sp, label='Class probability on Unlabeled set')
plt.show()
#plt.grid()
#plt.show()
~~~

